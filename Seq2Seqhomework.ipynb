{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrHOf5ajcv44C4pNEMnksj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdtlaura/nlp2/blob/main/Seq2Seqhomework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading T5 Model and Tokenizer"
      ],
      "metadata": {
        "id": "1Xl_3dWcMc8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This line imports two classes from the Hugging Face transformers library:\n",
        "\n",
        "T5Tokenizer: Responsible for converting text into tokens that the T5 model can understand.\n",
        "\n",
        "T5ForConditionalGeneration: A version of the T5 model designed for tasks like text generation, translation, summarization, and more.\n"
      ],
      "metadata": {
        "id": "_mWd41WnNUJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"t5-small\"  # You can replace this with any other model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "u_dPTL7tKOAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Gradio:\n",
        "Explanation: This command installs the Gradio library, which allows you to create simple, interactive interfaces for machine learning models.\n",
        "The -qqq option is used to suppress the usual installation output, keeping the notebook output cleaner."
      ],
      "metadata": {
        "id": "Myg0iLS3NEsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -qqq\n",
        "# https://www.gradio.app/"
      ],
      "metadata": {
        "id": "qQox_hj_Kpej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypr6hUC0kTUJ",
        "outputId": "3dd254f2-9e17-4fed-94f5-9971650a1ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.2/897.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the MarianMT Model for Translation with Gradio Interface"
      ],
      "metadata": {
        "id": "lTn7n0eFMmJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio: A library that simplifies the process of creating web interfaces for machine learning models.\n",
        "\n",
        "MarianMTModel and MarianTokenizer: Classes from Hugging Face's transformers library specifically for the MarianMT translation model, which is designed for machine translation tasks."
      ],
      "metadata": {
        "id": "7ys8qUQGNwUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Gradio UI Components:\n",
        "\n",
        "fn=translate: This specifies the function to be called when the user submits input. In our case, it's the translate() function defined earlier.\n",
        "\n",
        "inputs=\"text\": This creates an input box where users can type their English sentences.\n",
        "\n",
        "outputs=\"text\": This creates an output box where the translated text will be displayed.\n",
        "\n",
        "title and description: These are optional arguments that give your app a nice title and a description so users know what it does.\n"
      ],
      "metadata": {
        "id": "QKsRk_nwPGkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "P60mL25LJ5r4",
        "outputId": "f1403de4-c84e-49fe-d8b0-826ab45197ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://05b8469bab2cd67e93.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05b8469bab2cd67e93.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Load the MarianMT model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ar\"  # A model specifically trained for English to Arabic translation\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define the translation function\n",
        "def translate_to_english(input_text):\n",
        "    # Prepare the input text for the model\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate translation\n",
        "    output = model.generate(input_ids)\n",
        "\n",
        "    # Decode the output\n",
        "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=translate_to_english,  # The function for translation\n",
        "    inputs=\"text\",  # Input component (text box for user input)\n",
        "    outputs=\"text\",  # Output component (text box for the result)\n",
        "    title=\"English to Arabic Translator\",  # Title of the interface\n",
        "    description=\"Enter a English sentence, and this tool will translate it to Arabic using the specified model.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n",
        "\n",
        "# For other languages, refer to the MarianMT models available at:\n",
        "# https://huggingface.co/models?search=Helsinki-NLP\n",
        "# Example for English to French:\n",
        "# model_name = \"Helsinki-NLP/opus-mt-en-fr\"  # Replace with the appropriate model for desired language translation"
      ]
    }
  ]
}